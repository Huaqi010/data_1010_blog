{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification in multiple ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Classification is aimed to assign a document to one or more classes or categories. This classification has been applied to many fields, including news classification, spam filtering and sentiment analysis. One traditional way is called the Rule-based methods, which is based on a complex, artificial set of rules that requires human to participate in building, revising and updating rules. It has huge limitation on applying to different conditions and costs a lot of time and investment. With the development of technology, there are some automatic document classification techniques, which are based on machine learning and data driven methods. In this article, I will focus on exploring the application of Naive Bayes, Support Vector Machines, and Neutral Network on document classification.\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes is based on the independence assumptions of Bayes’ theorem. Simply, for a given input, Bayes’ theorem is used to find the output with the maximum posterior probability $P(Y|X)$, regardless any possible correlations between the features $X$. \n",
    "\n",
    "The aim of the classifier is to find the $Y$ which could get th maximum of $P(Y|X)$. \n",
    "\n",
    "**Assumptions:** \n",
    "\n",
    "· $X$ is the input observation  \n",
    "· $Y$ is the target(class) variable  \n",
    "· $R^n$ is a set of n-dimensional vectors  \n",
    "· $X \\subseteq R^n$ indicates that each observation has $n$ features.  \n",
    "· $T = \\{(x_1,y_1),..., (x_n, y_n)\\}$ is the train set.\n",
    "\n",
    "Based on the Bayes' thorem, given the featurs of the observation $x$, the probability of gitting $y_k$ is\n",
    "\n",
    "$$\n",
    "P(Y = y_k|X = x) = \\frac{P(Y = y_k)P(X = x|Y = y_k)}{\\sum_j{P(X = x|Y = y_j)}P(Y = y_j)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the independence assumptions, the conditional distribution ovr the class varirable $Y$ is \n",
    "\n",
    "$$\n",
    "P(Y = y_k|X = x) = \\frac{P(Y = y_k)\\prod_{i=1}^n{P(X = x|Y = y_k)}}{\\sum_jP(Y = y_j)\\prod_{i=1}^n{P(X = x|Y = y_j)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $P(Y = y_k|X = x) = max\\{P(y_1|x),...,P(y_n|x)\\}$, then the observation $x \\in y_k$. Thus, the classification model could be expressed by\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "y &= argmax_{y_k}P(Y = y_k|X = x) \\\\ \n",
    "&= argmax_{y_k}\\frac{P(Y = y_k)\\prod_{i=1}^n{P(X = x|Y = y_k)}}{\\sum_jP(Y = y_j)\\prod_{i=1}^n{P(X = x|Y = y_j)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because the equation is not dependent on the y_j, we could simplify the equation to\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "y &= argmax_{y_k}P(Y=y_k)\\prod_{i=1}^n{P(X = x|Y = y_k)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One significant attribute of text data is its large dimension. For example, a document may have thousands of wards. However, documents with different types or themes may have a large gap in terms of vocabulary, so the words appearing order can be ignored. The Naive Bayes method can be used to solve the document classification problem based on the hypothesis that every word in the document appears independently. I would like to show you how it works by the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](blog_imgs/NaiveB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the aim is to predict if the data in test set is in the class `China`. Firstly, I could get the prior probabilities:\n",
    "\n",
    "$$P(c) = 3/4, P(\\bar c) = 1/4$$\n",
    "\n",
    "Notice that, `Tokyo`, `Japan` is not appearing in the training set in class `China`, so the conditional probability of them would be 0, which might affect the prediction on test set. Thus, Laplace Smoothing is a good choice to calculate the conditional probabilities of each word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Chinese|c)=(5+1)/(8+6)=3/7\\\\\n",
    "P(Tokyo|c)=P(Japan|c)=(0+1)/(8+6)=1/14\\\\\n",
    "P(Chinese|\\bar c)=(1+1)/(3+6)=2/9 \\\\ \n",
    "P(Tokyo|\\bar c)=P(Japan|\\bar c)=(1+1)/(3+6)=2/9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we could get:\n",
    "\n",
    "$$\\begin{aligned} \n",
    "P(c|d5)  &\\propto  P(c)⋅P(Chinese|c)3⋅P(Tokyo|c)⋅P(Japan|c) \\\\\n",
    "&= \\frac{3}{4} · (\\frac{3}{7})^3 · \\frac{1}{14} · \\frac{1}{14} \\\\\n",
    "& \\approx 0.0003\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\begin{aligned} \n",
    "P(c|d5)  &\\propto  P(\\bar c)⋅P(Chinese|\\bar c)3⋅P(Tokyo|\\bar c)⋅P(Japan|\\bar c) \\\\\n",
    "&= \\frac{1}{4} · (\\frac{2}{9})^3 · \\frac{2}{9} · \\frac{2}{9} \\\\\n",
    "& \\approx 0.0001\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the Naive Bayes classifier assigns the test document to $c = China$. The reason for this classification decision is that the three occurrences of the positive indicator Chinese in d5 outweigh the occurrences of the two negative indicators Japan and Tokyo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier is based on Bayes’ theorem and adds independent assumption of each feature, which greatly reduces the parameter hypothesis space and simplifies the process of learning and prediction. Even the assumption of the Naive Bayes classifier seems to be simple, the performance on prediction is still good. One explanation from the article, `The Optimality of Naïve Bayes`, is that “naive Bayes may change the posterior probabilities of each class, but the class with the maximum posterior probability is often unchanged.” Although the correlation between features may be uniformly distributed in different categories, the correlation between different features may cancel each other out. Therefore, the classification could be still correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine is a popular method to do classification. It is based on Structural Risk Minimization principle, which would maximize the margin of decision hyperplane to reduce the risk of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When SVM applies to document classification, it would compresses the data set into support vector set, to learn the decision function. It is essentially a two-class classifier, which separate the data set by a linear separator, and then could be extended to multi-class data set and non-linear cases. In this section, I would start from the advantages of SVM in document classification, and then give a linearly separable case to illustrate the steps, and finaly discuss the feasibility in document classification with several categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The advantages of SVM to do document classification**\n",
    "\n",
    "SVM measures the complexity of the data set based on the hyperplane with which it separates the data, not the number of features. \n",
    "\n",
    "For example, if the data set with many features could be separate by one feature, then all the other features could be map to 0 and one point on the line could separate all the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM lower.png](blog_imgs/SVMlower.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data set with only one feature is not linearly separable, then all the data need to be projected to higher dimensional until they can be linearly separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM higher.png](blog_imgs/SVMhigher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in documents could be consider as features by vectoring the words or calculating their tf-idf , so that one document could have a lot of features. Also, text is often linearly separable. Using SVM could lower the parameters dimension which needs to be considered and increase the efficiency of doing classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linearly separable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two-class problems, there could be a lot of possible linear separatots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM linear.png](blog_imgs/SVMlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best decision surface, SVM defines a hyperplane by the following variables: \n",
    "\n",
    "· $b$ is an intercept term   \n",
    "· $w$ is the weight vector which is perpendicular to the hyperplane  \n",
    "· $\\{(x_i,y_i)\\}$ is the training set, where $x_i$ is a point and $y_i$ is the class lable to it  \n",
    "\n",
    "All points x on the hyperplane satisfy $w′x = -b$, and the two classes are always $+1$ and $-1$. Then the linear classifier could be defined as:\n",
    "\n",
    "$$f(x) = sign(w′x+b)$$\n",
    "\n",
    "When $w′x_i+b \\leq -1$, $y_i =-1$; when $w′x_i+b \\geq 1$, $y_i =1$. The spacing between consecutive integer level sets is $1/|w|$. Minimizing $|w|$ would maximize the margin of the hyperplane to get the best SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM theorem.png](blog_imgs/SVMtheorem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would agiain, use previous case to illustrate how SVM works in real cases. Firstly, I transfer the table into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese Beijing Chinese</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinese Chinese Shanghai</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinese Macao</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tokyo Japan Chinese</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words    c\n",
       "0   Chinese Beijing Chinese  yes\n",
       "1  Chinese Chinese Shanghai  yes\n",
       "2             Chinese Macao  yes\n",
       "3       Tokyo Japan Chinese   no"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'words':['Chinese Beijing Chinese', 'Chinese Chinese Shanghai', 'Chinese Macao', 'Tokyo Japan Chinese'],\n",
    "         'c': ['yes','yes','yes','no']}\n",
    "train_set =pd.DataFrame(data=d)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the data set into vectors by using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "tf_train_data = tfidf_transformer.fit_transform(train_set.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69183461, 0.722056  , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.722056  , 0.        , 0.        , 0.69183461,\n",
       "        0.        ],\n",
       "       [0.        , 0.46263733, 0.        , 0.88654763, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.34618161, 0.66338461, 0.        , 0.        ,\n",
       "        0.66338461]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_train_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the SVM model and do prediction on the test set. The result of the model matches the result of Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel = 'linear').fit(tf_train_data, train_set.c)\n",
    "test_set = ['Chinese Chinese Chinese Tokyo Japan']\n",
    "tf_docs_new = tfidf_transformer.transform(test_set)\n",
    "predicted = clf.predict(tf_docs_new)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, a document with high dimensional features could belong to several classes. When doing document classification, people expect to have a classifier which could deal with multi-class classification. It could be done by constructing several binary classifiers. Traditionally, there are two ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.`one-versus-rest classifiers`: also called “one-versus-all” or OVA classification. When training the classifier, the samples of a certain category, c_k, were classified into one category and the remaining samples into another. In this way, to classify the samples of k categories would require k SVMs, and the classifier would assign the data in test set to the class which gives it largest margin.\n",
    "\n",
    "2.`one-versus-one classifiers`: it builds a SVM between samples in any two categories. To classify samples of k categories would require to design k(k-1)/2 SVMs. When doing classification for an unseen sample in test set, the category with the most votes would be assigned to the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One experiment from the book, `Introduction to Information Retrieval`, shows the success results from Joachims(1998) of using SVM to do multiclass document classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM multi.png](blog_imgs/SVMmulti.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neutral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier and Support Vector Machine are both based on statistic models which require elaborate feature engineering. Neural Network uses the middle layer to get the appropriate feature automatically, so that the data input could be more primitive and dense, and keep the n-dimensional array structure in the network. It could not only consider the word as a feature, but also add the words relationship in the context as a feature to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network theorem\n",
    "\n",
    "Neutral Network is designed to be similar to nervous systems, which are used to discover complex patterns and relationships in tagged data. A shallow neural network consists of three layers of neurons: input layer, hidden layer and output layer. In document classification, the input would be the word vectors matrix which requires data preprocessing, the output layer would contains all the possible categories, and the input data would be transformed several times through the hidden layers to get the final classification.\n",
    "\n",
    "The idea is for each word in the document to generate a word vector, and then merge it into the matrix according to the sentence. This word vector matrix would be put into the neural network to classify the document. The simple neural network model could be built to predict previous example as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simple nn.png](blog_imgs/simple_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· Firslty, import Keras package and the training set shows as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese Beijing Chinese</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinese Chinese Shanghai</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinese Macao</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tokyo Japan Chinese</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words    c\n",
       "0   Chinese Beijing Chinese  yes\n",
       "1  Chinese Chinese Shanghai  yes\n",
       "2             Chinese Macao  yes\n",
       "3       Tokyo Japan Chinese   no"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· Then, preprocess the data, converting the target label into 1 and 0, and convert text data into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0], [1, 0], [1, 0], [0, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = [[1,0] if i == 'yes' else [0,1] for i in train_set.c.values ]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69183461, 0.722056  , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "tf_train_data.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.7420575, 0.4739993, 0.       , 0.       , 0.4739993]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['Chinese Chinese Chinese Tokyo Japan']\n",
    "tf_docs_new = tfidf_transformer.transform(test_set)\n",
    "tf_docs_new.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· Building the model with two hidden layer and one output layer, compiling the model by given the optimize method, loss function and evaluation metric. Fitting the training data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f824219f210>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols = tf_train_data.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "# Add the two hidden layer\n",
    "model.add(Dense(10, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2,activation = 'softmax'))\n",
    "                \n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(tf_train_data.toarray(), target, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· Putting the test data into the model and let it do prediction. The model would return the corresponding probabilities of two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5469802 , 0.45301974]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tf_docs_new.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results 0.56172836 represents the probability in class 1, corresponding to 'yes' category. Thus, the model gives the same result in previous two models, which classify the test data into `China` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model\n",
    "\n",
    "Convolution Neural Network is similiar to full coonection neural network. Each layer in CNN is made up of neurons. The difference is, rather than connecting each node in the neural network, only parts of the node is connected between adjacent layers in CNN. Thus, it could extract features in the documents to improve the accuracy in classification problems.\n",
    "\n",
    "CNN is usually composed of input layer, convolutional layer, pooling layer, full connection layer and softmax layer, which are all in the three dimension. The specific model is like following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn.png](blog_imgs/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer:** After the text in the document is passed to the input layer, it would convert to three-dimensional input with shape of $n*k*channel$. \n",
    "\n",
    "**Convolutional layer:** Then the input would be put into the convolutional layer. The size of the convolutional layer is $m*k*depth$, where $m$ is the length of the filter, $k$ is the dimension of embedding and $depth$ represents the depth of filter. Using different $m$ would involve different number of words to be convolved as features. For example, if $m = 1$, the convolutiional layer would return unigram feature; if $m = 2$, it would return bigram features. After passing through the convolutional layer, n-gram related features of the original text are extracted.\n",
    "\n",
    "\n",
    "**Pooling layer:** The max pooling method is used in pooling layer. The filter size of the pooling layer is $convLength *1$, where $convlength$ is  the length of the feature map after convolution of text. After pooling, the dimension of feature map will be reduced to 1, so after pooling, and the vector dimension would be $depth*filterNum$, whcih is the features that represent the whole document.\n",
    "\n",
    "**Full connection layer and softmax layer:** Finally, by using softmax, we could get the probability vectors representing different categories, and classify the document into the category with highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From the above reading, I hope you have an overview about how the three methods work well on document classification. Indeed, the three methods deal with text data with different assumptions and ways. \n",
    "\n",
    "Naive Bayes Classifier adds a causation between the inputs and outputs, which indicates the appearence of certain words could lead to the certain probability to put it into certain categories. Thus, it uses prior and posterior probability to do classification and it is easy to intepret. Unlike Naive Bayes, Support Vector Machine has no assumption, and it do classification just based on the data points and the distance between them. The data points would perform differently in higher dimension, and SVMs could separate these points and classify the closer data points into one category. Neutal Network largely depends on machine learning. It assumes there is a linear or non-linear function between the inputs and the outputs. It is hard to intepret because we, as human, cannot understand how machine learns from the data and find the function, which is called the 'black box' problem. Although you could find they all give the same results in my simple example, in reality, the results could be different because of the different ways they use to get the final prediction.\n",
    "\n",
    "Facing the complex problems and needs in document classification, there are more various ways could be used to do document classification. If you are interested in exploring more details, you could check the sources in my reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). Introduction to information retrieval. Cambridge: Cambridge University Press.\n",
    "\n",
    "Yoon Kim. (2014). Convolutional Neural Networks for Sentence Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
